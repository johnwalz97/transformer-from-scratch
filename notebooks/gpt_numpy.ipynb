{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformer_from_scratch import (\n",
    "    create_input_output_pairs,\n",
    "    create_vocabulary,\n",
    "    cross_entropy_loss,\n",
    "    load_dataset,\n",
    "    NumpyGPT,\n",
    "    one_hot_encode,\n",
    "    text_to_indices,\n",
    "    tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "file_path = \"dataset.txt\"\n",
    "text = load_dataset(file_path)\n",
    "tokens = tokenize(text)\n",
    "vocab, token_to_idx, idx_to_token = create_vocabulary(tokens)\n",
    "indices = text_to_indices(text, token_to_idx)\n",
    "input_data, output_data = create_input_output_pairs(indices, seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NumpyGPT model\n",
    "model = NumpyGPT(vocab_size=len(vocab), d_model=512, nhead=8, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (64,128,13331) (64,128,512) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(logits, targets)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m d_embedding, d_transformer_layers, d_fc_out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mbackward(logits, targets)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Update the model parameters\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model\u001b[39m.\u001b[39membedding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m d_embedding\n",
      "File \u001b[0;32m~/Code/Personal/transformer-from-scratch/transformer_from_scratch/numpy_gpt.py:78\u001b[0m, in \u001b[0;36mNumpyGPT.backward\u001b[0;34m(self, logits, targets)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m# compute gradients for each transformer layer\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers):\n\u001b[0;32m---> 78\u001b[0m     d_x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(d_logits)\n\u001b[1;32m     79\u001b[0m     d_transformer_layers\u001b[39m.\u001b[39mappend(layer\u001b[39m.\u001b[39mget_gradients())\n\u001b[1;32m     81\u001b[0m \u001b[39m# compute gradients for the embedding layer\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/Personal/transformer-from-scratch/transformer_from_scratch/numpy_gpt.py:128\u001b[0m, in \u001b[0;36mTransformerLayer.backward\u001b[0;34m(self, d_x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, d_x):\n\u001b[1;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform a backward pass through the layer.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m        d_x (np.ndarray): Gradient of the input tensor of shape (batch_size, seq_len, d_model).\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     d_ff_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2\u001b[39m.\u001b[39;49mbackward(d_x)\n\u001b[1;32m    129\u001b[0m     d_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward\u001b[39m.\u001b[39mbackward(d_ff_input)\n\u001b[1;32m    131\u001b[0m     d_att_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1\u001b[39m.\u001b[39mbackward(d_x)\n",
      "File \u001b[0;32m~/Code/Personal/transformer-from-scratch/transformer_from_scratch/numpy_gpt.py:292\u001b[0m, in \u001b[0;36mLayerNorm.backward\u001b[0;34m(self, d_x)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m*\u001b[39m_, D \u001b[39m=\u001b[39m d_x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    291\u001b[0m \u001b[39m# Compute the gradients for gamma and beta\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_gamma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(d_x \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_x, axis\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_beta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(d_x, axis\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    295\u001b[0m \u001b[39m# Compute the gradient for the input\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (64,128,13331) (64,128,512) "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_samples = input_data.shape[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the dataset\n",
    "    shuffle_indices = np.random.permutation(num_samples)\n",
    "    input_data = input_data[shuffle_indices]\n",
    "    output_data = output_data[shuffle_indices]\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        # Get the current batch\n",
    "        inputs = input_data[i:i + batch_size]\n",
    "        targets = output_data[i:i + batch_size]\n",
    "\n",
    "        # One-hot encode the inputs\n",
    "        inputs_one_hot = one_hot_encode(inputs, len(vocab))\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(inputs_one_hot)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = cross_entropy_loss(logits, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        d_embedding, d_transformer_layers, d_fc_out = model.backward(logits, targets)\n",
    "\n",
    "        # Update the model parameters\n",
    "        model.embedding -= learning_rate * d_embedding\n",
    "        model.fc_out -= learning_rate * d_fc_out\n",
    "\n",
    "        # Update the transformer layers\n",
    "        for layer, (d_layer, d_att, d_norm1, d_norm2, d_ff) in zip(model.transformer_layers, d_transformer_layers):\n",
    "            layer.attention.W_q -= learning_rate * d_att[0]\n",
    "            layer.attention.W_k -= learning_rate * d_att[1]\n",
    "            layer.attention.W_v -= learning_rate * d_att[2]\n",
    "            layer.norm1.gamma -= learning_rate * d_norm1[0]\n",
    "            layer.norm1.beta -= learning_rate * d_norm1[1]\n",
    "            layer.norm2.gamma -= learning_rate * d_norm2[0]\n",
    "            layer.norm2.beta -= learning_rate * d_norm2[1]\n",
    "            layer.feed_forward.fc1 -= learning_rate * d_ff[0]\n",
    "            layer.feed_forward.fc2 -= learning_rate * d_ff[1]\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{num_samples}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch-5bReXo1X-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
